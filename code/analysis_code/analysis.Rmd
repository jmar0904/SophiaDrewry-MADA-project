---
title: "Analysis"
author: "Sophia Drewry"
date: "10/28/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
load needed packages. make sure they are installed.

```{r}
library(readr)
library(dplyr) #for data processing
library(here) #to set paths
library(ggthemes)
library(RColorBrewer)
library(ggplot2)
library(table1)
library(lubridate)
library(tidymodels)
library(forecast)  # for `auto.arima`
library(timetk)    # for `tk_ts`
library(zoo) 
library(poissonreg)
library(gam)
#Load data
data_spot1 <- here::here("data","processed_data","Finaldata.rds")
#load data 
FINALdta <- read_rds(data_spot1)
```
# Pre-processing
Creating Weekly Incidence Rate variable
```{r}
# Creating Weekly Incidence Rate variable so we can compare across years
str(FINALdta)
FINALdta <- FINALdta %>% mutate(IR = (Total/Estimated_population)  * 100000)
FINALdta$WeekDate <- as.Date(as.character(FINALdta$WeekDate))
# From EDA we could see that case counts do not follow a normal distribution 
FINALdta2 <- FINALdta
FINALdta2$Total <- log(FINALdta2$Total)
hist(FINALdta$Total,col='red')
hist(FINALdta2$Total,col='red')
# Based off the information from this graph, I want to set a natural log of IR for our model dataset
# Create a new set with only predictor variables, outcome and date
model.data <- FINALdta[c("WeekDate", "Season", "IR", "Total", "MinAT","MaxAT", "Precip", "TAvg", "SOI", "NINO4", "NINO3.4", "ENSO")]
ts.dta <- FINALdta[c("Year", "Month", "IR","MaxAT", "Precip", "TAvg", "NINO4", "SOI", "NINO3.4", "ENSO")]
```
##  Adjusting for Seasonality
source: https://rpubs.com/davoodastaraky/TSA1
Here we are going to run some quick bivariate comparisons between dengue case numbers
More importantly we are looking at graphical patterns to determine lag period 
```{r}
# Transforming into a time series 
adj <- ts(ts.dta, start=c(2000, 7), end=c(2013, 6), frequency=12)
plot.ts(adj)
# looking at seasonal decomposition, multiplicative effects
# Decomposing Seasonal Data
data.comp <- decompose(adj)
data.comp # cannot use plot function because there are more than 10 predictors
# now lets look at each predictor adjusted
adj.model <- adj - data.comp$seasonal
plot.ts(adj.model)
```

## Data splitting
Here we are going to split the data for testing and training models. We are looking at "seasons" instead of "years" in order to get a better understand of dengue season patterns. 
- Training data will be used to fit the model. 2000/2001- 2008/2009 season  
- Testing set will be used to evaluate the model. 2009/2010- 2012/2013 season
* note that the date 2009-07-02 cuts off the 2008/2009 to the 2009/2010 season
Source: https://www.tidymodels.org/learn/models/time-series/
```{r}
# Setting a seed for random number generation so if this analysis is reproduced, the same random set will be generated
# Basic way to splitting data
train.dta<- model.data %>%filter(WeekDate < as.Date("2009-07-02"))
test.dta<- model.data %>%filter(WeekDate >= as.Date("2009-07-02"))

# Tidy models way for time series data
# used in ARIMA tutorial, using a copy set to preserve data set for future models
resample <- rolling_origin(
  model.data, 
  initial = 469, #  2009-07-02 is the 469th observation
  assess = 52, # Because there are 52 weeks per year/season starting from start date...?t
  cumulative = FALSE)
  #lag = 14) # average time between nino event and inc in cases. I don't think I want this here for now.
nrow(resample )
#> 17 rows
resample
resample$splits[[1]]
#<Analysis/Assess/Total>
#<469/56/677>
```
For plotting,index each split by the first day of the assessment set. Not sure if I need to do this anymore... if we are offsetting. 
get_date <- function(x) {min(assessment(x))}
start_date <- map(resample$splits, get_date)
resample$start_date <- do.call("c", start_date)
head(resample$start_date)

# EDA Modeling
## Weather Predictors vs. Dengue IR
Weather predictors are as follows:
  From station data
- MIN & MAT Air Temp (min) /// cont.
- Precipitation /// cont.
- Average Temp /// cont.
  From NOAA
- ENSO // categorical
- SOI /// cont.
- SST (region 3.4) /// cont.


## Looking at Station data predictors
```{r}
# Setting engine as LM because all are continuous
glm.mod <- linear_reg() %>% 
 set_engine("lm") %>%
  set_mode("regression")
# Setting recipe
weather.rec<- recipe(IR ~ MinAT + MaxAT + Precip + TAvg, data = train.dta)
# workflow
weather.wflow <- 
  workflow() %>% 
  add_model(glm.mod) %>% 
  add_recipe(weather.rec)
weather.wflow
# fitting data
weather.fit <- 
  weather.wflow %>% 
  fit(data = FINALdta)
weather.fit
# To view a tibble 
weather.fit %>% extract_fit_parsnip() %>% tidy( )
```
## Evaluation
```{r}
predict(weather.fit, train.dta)
# So we are going to "augment" the test data function to show the predictors individually
weather.aug <- augment(weather.fit, train.dta)
weather.aug %>% select(IR, .pred) 
```


////////////////////////////////////////////////////////////////////////////////
# Analysis Models


## Create a null model to compare models
##### Creating a Null Model ####
This is to use as a comparison for our other future models
```{r}
# creating model type
glm.mod<- poisson_reg() %>%
  set_engine("glm") %>%
  translate()
# Create null formula
n.rec <- recipe(IR ~ 1., data = train.dta)  
# Training model
# Creating null recipe & model with TRAIN data
# set workflow
N.train.wflow <-
  workflow() %>% 
  add_model(glm.mod) %>% 
  add_recipe(n.rec)
# fitting
N.train.fit <- 
  N.train.wflow %>% 
  fit(data = train.dta)
# usual
N.train.fit %>% 
  extract_fit_parsnip() %>% 
  tidy()
# RMSE
predict(N.train.fit, train.dta)
N.train.aug <- augment(N.train.fit, train.dta)
N.train.aug %>% select(IR, .pred) 
N.train.aug %>% rmse(truth = IR, .pred)
# RMSE = 2.660016		
```

## Multiple Linear Regression Model ###########################################
Sources:
https://www.tidymodels.org/learn/models/time-series/

```{r}
# to get a look at seasonal variation
ggplot(FINALdta, aes(Season, IR)) +
  geom_jitter(height = 0)

# specifying the model using the poisson_reg() function
pois.spec <- poisson_reg() %>% 
  set_mode("regression") %>% 
  set_engine("glm")
#create dummy variable
pois.rec <- recipe(IR ~ MinAT + MaxAT + Precip + TAvg + SOI + NINO4 + NINO3.4 + ENSO,
  data = train.dta) %>% 
  step_dummy(all_nominal_predictors())
#workflow
pois.wf <- workflow() %>% 
  add_recipe(pois.rec) %>% 
  add_model(pois.spec)

#fit the model and look at the predictions.
pois.fit <- pois.wf %>% fit(data = train.dta)
pois.fit %>% extract_fit_parsnip() %>% tidy()

# RMSE
predict(pois.fit, train.dta)
pois.aug <- augment(pois.fit, train.dta)
pois.aug %>% select(IR, .pred) 
pois.aug %>% rmse(truth = IR, .pred)
# RMSE = 2.490726	

# graph
augment(pois.fit, new_data = train.dta, type.predict = "response") %>% 
  ggplot(aes(IR, .pred)) +
  geom_point(alpha = 0.1) +
  geom_abline(slope = 1, size = 1, color = "grey40") +
  labs(title = "Predicting the degue incidence rate using Poission Regression",
       x = "Actual", y = "Predicted")
```
### Evaluation
```{r}
pois.fit.coes <- 
  tidy(pois.fit) 

pois.fit.coes %>% 
  ggplot(aes(term, estimate)) +
  geom_line(group = 1) +
  geom_point(shape = 21, size = 3, stroke = 1.5, 
             fill = "black", color = "white") +
  labs(title = "Coefficient value from Poission Regression",
       x = "Month", y = "Coefficient")

```

## GAM Model #################################################################
source: https://m-clark.github.io/generalized-additive-models/application.html#multiple-predictors

```{r}
# create model
gam.lm <- gam(IR ~ MinAT + MaxAT + Precip + TAvg + Estimated_population + SOI + NINO4 + NINO3.4 + ENSO,
  data = train.dta, family = poisson)
summary(gam.lm) #taking out estimated population because that is included in IR. Another option would be to look at case counts and include population as a predictor
gam.lm2 <- gam(IR ~ MinAT + MaxAT + Precip + TAvg + SOI + NINO4 + NINO3.4 + ENSO,
  data = train.dta, family = poisson)
summary(gam.lm2)
# 4 predictors stand out, in order of strongest as follows:
# MinAT, MaxAT, SOI and ENSO
```
Now lets try a couple more using those 
```{r}
# try a smoothed as well
gam.lm3 <- gam(IR ~ s(MinAT) + s(MaxAT) + (SOI) + (ENSO), # can only smooth 3 variables
  data = train.dta, family = poisson) 
summary(gam.lm3) #Enso performed better than expects
# ok try with MaxAT not smooth
gam.lm4 <- gam(IR ~ s(MinAT) + MaxAT + s(NINO3.4),
  data = train.dta, family = poisson)
summary(gam.lm4) # MaxAT is almost 0 even not smoothed
# getting error messages because the IR is a non-integer
```
## visualization
```{r}
testdata = data.frame(MinAT = seq(.4, 1, length = 100),
                      SOI = mean(mod_gam2$model$Edu),
                      ENSO = mean(mod_gam2$model$Health))
fits = predict(mod_gam2, newdata=testdata, type='response', se=T)
predicts = data.frame(testdata, fits) %>% 
  mutate(lower = fit - 1.96*se.fit,
         upper = fit + 1.96*se.fit)

plot_mod_gam2_response = ggplot(aes(x=Income,y=fit), data=predicts) +
  geom_ribbon(aes(ymin = lower, ymax=upper), fill='gray90') +
  geom_line(color='#00aaff') +
  theme_trueMinimal()
```

## SARIMA #################################################################
source:https://www.tidymodels.org/learn/models/time-series/
```{r}
lag.plot(model.data,lags=12,do.lines=FALSE)

ts.train = ts(train.dta)
features = c("MinAT", "MaxAT", "Precip", "TAvg", "SOI", "NINO4", "NINO3.4", "ENSO")
arima_fit = auto.arima(ts.train[features])


cases.lm = lm(log(Total + 1) ~ MinAT + MaxAT + Precip + TAvg + SOI + NINO4 + NINO3.4 + ENSO, data = model.data)
cases.lm.resid = resid(cases.lm)
par(mfrow=c(1,3))
plot(model.data$MinAT, cases.lm.resid,
     ylab="Residuals", xlab="Min Air Temp", 
     main="Residuals vs. Min Air Temp")
abline(0,0)
plot(train.dta$MaxAT, cases.lm.resid,
     ylab="Residuals", xlab="Max Air Temp", 
     main="Residuals vs. Max Air Temp")
abline(0,0)
# autocorrelation plot
acf(cases.lm.resid, lag = 100, main = 'Autocorrelation of residuals')

```
# a little but of pre processing
# this code is from tidymodel tutorial, creating model and starting it on specified date
fit_model <- function(x, ...) {x %>%
    analysis() %>%
    tk_ts(start = .$date[[1]] %>% as.yearmon(), 
          frequency = 12, 
          silent = TRUE) %>%
    auto.arima(...)}
#Save each model in a new column:
# but first making a copy of model data for future models
```

```{r}
get_date <- function(x) {
  min(assessment(x)$StartDate)}

start_date <- map(resample$splits, get_date)
resample$start_date <- do.call("c", start_date)
head(roll_rs$start_date)
resample$arima <- map(resample$splits, fit_model)
```
